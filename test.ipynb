{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/run-dong-zhu/Machine-Learning-Practice/blob/master/test.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "WwFnh4uXYq9W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "a6541b38-17ea-40de-cc78-dade92375713"
      },
      "cell_type": "code",
      "source": [
        "!wget http://www.robots.ox.ac.uk/~vgg/data/fgvc-aircraft/archives/fgvc-aircraft-2013b.tar.gz"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-04-10 23:05:01--  http://www.robots.ox.ac.uk/~vgg/data/fgvc-aircraft/archives/fgvc-aircraft-2013b.tar.gz\n",
            "Resolving www.robots.ox.ac.uk (www.robots.ox.ac.uk)... 129.67.94.2\n",
            "Connecting to www.robots.ox.ac.uk (www.robots.ox.ac.uk)|129.67.94.2|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2753340328 (2.6G) [application/x-gzip]\n",
            "Saving to: ‘fgvc-aircraft-2013b.tar.gz.1’\n",
            "\n",
            "rcraft-2013b.tar.gz   0%[                    ]  25.30M   599KB/s    eta 71m 27s"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "fgvc-aircraft-2013b  99%[==================> ]   2.56G   922KB/s    eta 0s     "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rfgvc-aircraft-2013b 100%[===================>]   2.56G   916KB/s    in 53m 42s \r\n",
            "\r\n",
            "2018-04-10 23:58:43 (835 KB/s) - ‘fgvc-aircraft-2013b.tar.gz.1’ saved [2753340328/2753340328]\r\n",
            "\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uynL41aNADDy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!rm *.h5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fG-37yUDB3zS",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "b8a009a4-616d-4885-9055-e00a745ee630"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-78c0d6bb-3000-4135-96ff-a2764eab83e8\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-78c0d6bb-3000-4135-96ff-a2764eab83e8\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving train_data.txt to train_data.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KoM6AlZFCpEW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "c6c5608e-a6c5-4b1a-ab8f-38fad7927cda"
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bcnn_DD_woft.py\t\t    fgvc-aircraft-2013b.tar.gz.1  train_data.txt\r\n",
            "datalab\t\t\t    index.html\t\t\t  validation_data.txt\r\n",
            "fgvc-aircraft-2013b\t    index.html.1\t\t  vgg16_weights.npz\r\n",
            "fgvc-aircraft-2013b.tar.gz  test_data.txt\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1jNhvpOWErgx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        },
        "outputId": "b4f0a882-beb3-4b61-d027-159eb1e06120"
      },
      "cell_type": "code",
      "source": [
        "!pip install tflearn"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tflearn\n",
            "  Downloading tflearn-0.3.2.tar.gz (98kB)\n",
            "\u001b[K    100% |████████████████████████████████| 102kB 2.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from tflearn)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tflearn)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tflearn)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow->tflearn)\n",
            "Building wheels for collected packages: tflearn\n",
            "  Running setup.py bdist_wheel for tflearn ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/fb/06/72/0478c938ca315c6fddcce8233b80ec91a115ce4496a27e8c90\n",
            "Successfully built tflearn\n",
            "Installing collected packages: tflearn\n",
            "Successfully installed tflearn-0.3.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-609fde011d8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# image_shape option can be set to different values to create images of different sizes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mbuild_hdf5_image_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'file'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'new_val_224.h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategorical_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Done creating new_val.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mbuild_hdf5_image_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'file'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'new_test_224.h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategorical_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'build_hdf5_image_dataset' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "0Xm504GaE8eN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "b31e8fc0-b416-4b69-c3ac-3283ab0698e1"
      },
      "cell_type": "code",
      "source": [
        "from tflearn.data_utils import build_hdf5_image_dataset\n",
        "import h5py\n",
        "\n",
        "new_train = \"./train_data.txt\"\n",
        "\n",
        "new_val = \"./validation_data.txt\"\n",
        "new_test = \"./test_data.txt\"\n",
        "\n",
        "# image_shape option can be set to different values to create images of different sizes\n",
        "build_hdf5_image_dataset(new_val, image_shape=(224, 224), mode='file', output_path='new_val_224.h5', categorical_labels=True, normalize=False)\n",
        "print('Done creating new_val.h5')\n",
        "build_hdf5_image_dataset(new_test, image_shape=(224, 224), mode='file', output_path='new_test_224.h5', categorical_labels=True, normalize=False)\n",
        "print('Done creating new_test.h5')\n",
        "\n",
        "build_hdf5_image_dataset(new_train, image_shape=(488, 488), mode='file', output_path='new_train_488.h5', categorical_labels=True, normalize=False)\n",
        "print('Done creating new_train_488.h5')"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done creating new_val.h5\n",
            "Done creating new_test.h5\n",
            "Done creating new_train_488.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3ddvNZNMzGSV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "1e568451-e2f3-4155-880b-2ed3bde8d971"
      },
      "cell_type": "code",
      "source": [
        "!ls -la"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 11062596\r\n",
            "drwxr-xr-x 1 root root        4096 Apr 11 03:24 .\r\n",
            "drwxr-xr-x 1 root root        4096 Apr 10 21:51 ..\r\n",
            "-rw-r--r-- 1 root root       18882 Apr 11 01:00 bcnn_DD_woft.py\r\n",
            "drwx------ 4 root root        4096 Apr 10 22:09 .cache\r\n",
            "drwxr-xr-x 3 root root        4096 Apr 10 22:09 .config\r\n",
            "drwxr-xr-x 1 root root        4096 Mar 13 21:48 datalab\r\n",
            "drwxr-xr-x 3  978 users       4096 Jan 17  2014 fgvc-aircraft-2013b\r\n",
            "-rw-r--r-- 1 root root  2753340328 Jan 17  2014 fgvc-aircraft-2013b.tar.gz\r\n",
            "-rw-r--r-- 1 root root  2753340328 Jan 17  2014 fgvc-aircraft-2013b.tar.gz.1\r\n",
            "drwxr-xr-x 4 root root        4096 Apr 10 21:52 .forever\r\n",
            "-rw-r--r-- 1 root root       13806 Jan 17  2014 index.html\r\n",
            "-rw-r--r-- 1 root root       13806 Jan 17  2014 index.html.1\r\n",
            "drwxr-xr-x 5 root root        4096 Apr 10 22:09 .ipython\r\n",
            "drwxr-xr-x 2 root root        4096 Apr 11 00:22 .keras\r\n",
            "drwx------ 3 root root        4096 Apr 10 21:52 .local\r\n",
            "-rw-r--r-- 1 root root  2008174640 Apr 11 03:24 new_test_224.h5\r\n",
            "-rw-r--r-- 1 root root  2858130144 Apr 11 03:25 new_train_488.h5\r\n",
            "-rw-r--r-- 1 root root   401275136 Apr 11 03:23 new_val_224.h5\r\n",
            "drwx------ 3 root root        4096 Apr 11 01:37 .nv\r\n",
            "-rw------- 1 root root        1024 Apr 10 21:52 .rnd\r\n",
            "-rw-r--r-- 1 root root      162984 Apr 11 00:34 test_data.txt\r\n",
            "-rw-r--r-- 1 root root       48884 Apr 11 03:11 train_data.txt\r\n",
            "-rw-r--r-- 1 root root       32562 Apr 11 00:34 validation_data.txt\r\n",
            "-rw-r--r-- 1 root root   553436134 Nov 13  2016 vgg16_weights.npz\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "scNL-X4_TiCY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "c2c1373e-e35b-495c-9e88-856d5d9642e7"
      },
      "cell_type": "code",
      "source": [
        "!python bcnn_DD_woft.py"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n",
            "  from ._conv import register_converters as _register_converters\n",
            "--Data Preprocessing--\n",
            "Input data read complete\n",
            "Data shapes -- (train, val, test) (4001, 488, 488, 3) (666, 224, 224, 3)\n",
            "tcmalloc: large alloc 11433771008 bytes == 0x5a30000 @  0x7fe96afcb107 0x7fe9688bd9a1 0x7fe9689209a3 0x7fe96892272e 0x7fe9689baa08 0x4c4b0b 0x54f3c4 0x551ee0 0x54efc1 0x54ffee 0x48b786 0x7fe9226928a5 0x459af1 0x45a38b 0x7fe96891fe14 0x7fe96892401e 0x7fe968924395 0x7fe9689bb4c1 0x4c4b0b 0x54f3c4 0x553aaf 0x54efc1 0x54f24d 0x553aaf 0x54efc1 0x54ff73 0x42b3c9 0x42b5b5 0x44182b 0x421f64 0x7fe969e141c1\n",
            "tcmalloc: large alloc 11433771008 bytes == 0x2afafa000 @  0x7fe96afcb107 0x7fe9688bd9a1 0x7fe968923550 0x7fe968923724 0x7fe968923c3d 0x7fe968924395 0x7fe9689bb4c1 0x4c4b0b 0x54f3c4 0x553aaf 0x54efc1 0x54f24d 0x553aaf 0x54efc1 0x54ff73 0x42b3c9 0x42b5b5 0x44182b 0x421f64 0x7fe969e141c1 0x42201a (nil)\n",
            "Killed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AfIDhobsULrJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "This file is used for the first step of the training procedure of Bilinear_CNN\n",
        "where only last layer of the Bilinear_CNN (DD) model is trained. \n",
        "Two VGG16 networks are connected at the output of conv5_3 layer to form \n",
        "a Bilinear_CNN (DD) network and bilinear merging is performed on connect \n",
        "these two convolutional layers.\n",
        "No finetuning is performed on the convolutional layers.\n",
        "Only blinear layers are trained in this first step.\n",
        "'''\n",
        "\n",
        "from __future__ import print_function\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import tflearn\n",
        "from tflearn.data_preprocessing import ImagePreprocessing\n",
        "from tflearn.data_augmentation import ImageAugmentation\n",
        "import os\n",
        "from tflearn.data_utils import shuffle\n",
        "\n",
        "import pickle \n",
        "from tflearn.data_utils import image_preloader\n",
        "import h5py\n",
        "import math\n",
        "import logging\n",
        "import random\n",
        "import time\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def random_flip_right_to_left(image_batch):\n",
        "    result = []\n",
        "    for n in range(image_batch.shape[0]):\n",
        "        if bool(random.getrandbits(1)):\n",
        "            result.append(image_batch[n][:,::-1,:])\n",
        "        else:\n",
        "            result.append(image_batch[n])\n",
        "    return result\n",
        "\n",
        "class vgg16:\n",
        "    def __init__(self, imgs, weights=None, sess=None):\n",
        "        self.imgs = imgs\n",
        "        self.last_layer_parameters = []     ## Parameters in this list will be optimized when only last layer is being trained \n",
        "        self.parameters = []                ## Parameters in this list will be optimized when whole BCNN network is finetuned\n",
        "        self.convlayers()                   ## Create Convolutional layers\n",
        "        self.fc_layers()                    ## Create Fully connected layer\n",
        "        self.weight_file = weights          \n",
        "        #self.load_weights(weights, sess)\n",
        "\n",
        "\n",
        "    def convlayers(self):\n",
        "        \n",
        "        # zero-mean input\n",
        "        with tf.name_scope('preprocess') as scope:\n",
        "            mean = tf.constant([123.68, 116.779, 103.939], dtype=tf.float32, shape=[1, 1, 1, 3], name='img_mean')\n",
        "            images = self.imgs-mean\n",
        "            print('Adding Data Augmentation')\n",
        "            \n",
        "\n",
        "        # conv1_1\n",
        "        with tf.name_scope('conv1_1') as scope:\n",
        "            kernel = tf.Variable(tf.truncated_normal([3, 3, 3, 64], dtype=tf.float32,\n",
        "                                                     stddev=1e-1), trainable=False, name='weights')\n",
        "            conv = tf.nn.conv2d(images, kernel, [1, 1, 1, 1], padding='SAME')\n",
        "            biases = tf.Variable(tf.constant(0.0, shape=[64], dtype=tf.float32),\n",
        "                                   trainable=False, name='biases')\n",
        "            out = tf.nn.bias_add(conv, biases)\n",
        "            self.conv1_1 = tf.nn.relu(out, name=scope)\n",
        "            self.parameters += [kernel, biases]\n",
        "\n",
        "        # conv1_2\n",
        "        with tf.name_scope('conv1_2') as scope:\n",
        "            kernel = tf.Variable(tf.truncated_normal([3, 3, 64, 64], dtype=tf.float32,\n",
        "                                                     stddev=1e-1), trainable=False, name='weights')\n",
        "            conv = tf.nn.conv2d(self.conv1_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
        "            biases = tf.Variable(tf.constant(0.0, shape=[64],  dtype=tf.float32),\n",
        "                                   trainable=False, name='biases')\n",
        "            out = tf.nn.bias_add(conv, biases)\n",
        "            self.conv1_2 = tf.nn.relu(out, name=scope)\n",
        "            self.parameters += [kernel, biases]\n",
        "\n",
        "        # pool1\n",
        "        self.pool1 = tf.nn.max_pool(self.conv1_2,\n",
        "                               ksize=[1, 2, 2, 1],\n",
        "                               strides=[1, 2, 2, 1],\n",
        "                               padding='SAME',\n",
        "                               name='pool1')\n",
        "\n",
        "        # conv2_1\n",
        "        with tf.name_scope('conv2_1') as scope:\n",
        "            kernel = tf.Variable(tf.truncated_normal([3, 3, 64, 128], dtype=tf.float32,\n",
        "                                                     stddev=1e-1), trainable=False,  name='weights')\n",
        "            conv = tf.nn.conv2d(self.pool1, kernel, [1, 1, 1, 1], padding='SAME')\n",
        "            biases = tf.Variable(tf.constant(0.0, shape=[128], dtype=tf.float32),\n",
        "                                   trainable=False, name='biases')\n",
        "            out = tf.nn.bias_add(conv, biases)\n",
        "            self.conv2_1 = tf.nn.relu(out, name=scope)\n",
        "            self.parameters += [kernel, biases]\n",
        "\n",
        "        # conv2_2\n",
        "        with tf.name_scope('conv2_2') as scope:\n",
        "            kernel = tf.Variable(tf.truncated_normal([3, 3, 128, 128], dtype=tf.float32,\n",
        "                                                     stddev=1e-1), trainable=False,  name='weights')\n",
        "            conv = tf.nn.conv2d(self.conv2_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
        "            biases = tf.Variable(tf.constant(0.0, shape=[128], dtype=tf.float32), trainable=False, name='biases')\n",
        "            out = tf.nn.bias_add(conv, biases)\n",
        "            self.conv2_2 = tf.nn.relu(out, name=scope)\n",
        "            self.parameters += [kernel, biases]\n",
        "\n",
        "        # pool2\n",
        "        self.pool2 = tf.nn.max_pool(self.conv2_2,\n",
        "                               ksize=[1, 2, 2, 1],\n",
        "                               strides=[1, 2, 2, 1],\n",
        "                               padding='SAME',\n",
        "                               name='pool2')\n",
        "\n",
        "        # conv3_1\n",
        "        with tf.name_scope('conv3_1') as scope:\n",
        "            kernel = tf.Variable(tf.truncated_normal([3, 3, 128, 256], dtype=tf.float32,\n",
        "                                                     stddev=1e-1),  trainable=False, name='weights')\n",
        "            conv = tf.nn.conv2d(self.pool2, kernel, [1, 1, 1, 1], padding='SAME')\n",
        "            biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32),\n",
        "                                   trainable=False, name='biases')\n",
        "            out = tf.nn.bias_add(conv, biases)\n",
        "            self.conv3_1 = tf.nn.relu(out, name=scope)\n",
        "            self.parameters += [kernel, biases]\n",
        "\n",
        "        # conv3_2\n",
        "        with tf.name_scope('conv3_2') as scope:\n",
        "            kernel = tf.Variable(tf.truncated_normal([3, 3, 256, 256], dtype=tf.float32,\n",
        "                                                     stddev=1e-1), trainable=False,  name='weights')\n",
        "            conv = tf.nn.conv2d(self.conv3_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
        "            biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32),\n",
        "                                   trainable=False, name='biases')\n",
        "            out = tf.nn.bias_add(conv, biases)\n",
        "            self.conv3_2 = tf.nn.relu(out, name=scope)\n",
        "            self.parameters += [kernel, biases]\n",
        "\n",
        "        # conv3_3\n",
        "        with tf.name_scope('conv3_3') as scope:\n",
        "            kernel = tf.Variable(tf.truncated_normal([3, 3, 256, 256], dtype=tf.float32,\n",
        "                                                     stddev=1e-1),  trainable=False,  name='weights')\n",
        "            conv = tf.nn.conv2d(self.conv3_2, kernel, [1, 1, 1, 1], padding='SAME')\n",
        "            biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32),\n",
        "                                   trainable=False, name='biases')\n",
        "            out = tf.nn.bias_add(conv, biases)\n",
        "            self.conv3_3 = tf.nn.relu(out, name=scope)\n",
        "            self.parameters += [kernel, biases]\n",
        "\n",
        "        # pool3\n",
        "        self.pool3 = tf.nn.max_pool(self.conv3_3,\n",
        "                               ksize=[1, 2, 2, 1],\n",
        "                               strides=[1, 2, 2, 1],\n",
        "                               padding='SAME',\n",
        "                               name='pool3')\n",
        "\n",
        "        # conv4_1\n",
        "        with tf.name_scope('conv4_1') as scope:\n",
        "            kernel = tf.Variable(tf.truncated_normal([3, 3, 256, 512], dtype=tf.float32,\n",
        "                                                     stddev=1e-1), trainable=False,  name='weights')\n",
        "            conv = tf.nn.conv2d(self.pool3, kernel, [1, 1, 1, 1], padding='SAME')\n",
        "            biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n",
        "                                  trainable=False, name='biases')\n",
        "            out = tf.nn.bias_add(conv, biases)\n",
        "            self.conv4_1 = tf.nn.relu(out, name=scope)\n",
        "            self.parameters += [kernel, biases]\n",
        "\n",
        "        # conv4_2\n",
        "        with tf.name_scope('conv4_2') as scope:\n",
        "            kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\n",
        "                                                     stddev=1e-1), trainable=False,   name='weights')\n",
        "            conv = tf.nn.conv2d(self.conv4_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
        "            biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n",
        "                                  trainable=False, name='biases')\n",
        "            out = tf.nn.bias_add(conv, biases)\n",
        "            self.conv4_2 = tf.nn.relu(out, name=scope)\n",
        "            self.parameters += [kernel, biases]\n",
        "\n",
        "        # conv4_3\n",
        "        with tf.name_scope('conv4_3') as scope:\n",
        "            kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\n",
        "                                                     stddev=1e-1), trainable=False,  name='weights')\n",
        "            conv = tf.nn.conv2d(self.conv4_2, kernel, [1, 1, 1, 1], padding='SAME')\n",
        "            biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n",
        "                                  trainable=False, name='biases')\n",
        "            out = tf.nn.bias_add(conv, biases)\n",
        "            self.conv4_3 = tf.nn.relu(out, name=scope)\n",
        "            self.parameters += [kernel, biases]\n",
        "\n",
        "        # pool4\n",
        "        self.pool4 = tf.nn.max_pool(self.conv4_3,\n",
        "                               ksize=[1, 2, 2, 1],\n",
        "                               strides=[1, 2, 2, 1],\n",
        "                               padding='SAME',\n",
        "                               name='pool4')\n",
        "\n",
        "        # conv5_1\n",
        "        with tf.name_scope('conv5_1') as scope:\n",
        "            kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\n",
        "                                                     stddev=1e-1),  trainable=False, name='weights')\n",
        "            conv = tf.nn.conv2d(self.pool4, kernel, [1, 1, 1, 1], padding='SAME')\n",
        "            biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n",
        "                                  trainable=False, name='biases')\n",
        "            out = tf.nn.bias_add(conv, biases)\n",
        "            self.conv5_1 = tf.nn.relu(out, name=scope)\n",
        "            self.parameters += [kernel, biases]\n",
        "\n",
        "        # conv5_2\n",
        "        with tf.name_scope('conv5_2') as scope:\n",
        "            kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\n",
        "                                                     stddev=1e-1), trainable=False,  name='weights')\n",
        "            conv = tf.nn.conv2d(self.conv5_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
        "            biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n",
        "                                  trainable=False, name='biases')\n",
        "            out = tf.nn.bias_add(conv, biases)\n",
        "            self.conv5_2 = tf.nn.relu(out, name=scope)\n",
        "            self.parameters += [kernel, biases]\n",
        "\n",
        "        # conv5_3\n",
        "        with tf.name_scope('conv5_3') as scope:\n",
        "            kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\n",
        "                                                     stddev=1e-1), trainable=False,  name='weights')\n",
        "            conv = tf.nn.conv2d(self.conv5_2, kernel, [1, 1, 1, 1], padding='SAME')\n",
        "            biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n",
        "                                  trainable=False, name='biases')\n",
        "            out = tf.nn.bias_add(conv, biases)\n",
        "            self.conv5_3 = tf.nn.relu(out, name=scope)\n",
        "            self.parameters += [kernel, biases]\n",
        "\n",
        "        print('Shape of conv5_3', self.conv5_3.get_shape())\n",
        "        self.phi_I = tf.einsum('ijkm,ijkn->imn',self.conv5_3,self.conv5_3)\n",
        "        print('Shape of phi_I after einsum', self.phi_I.get_shape())\n",
        "\n",
        "        \n",
        "        self.phi_I = tf.reshape(self.phi_I,[-1,512*512])\n",
        "        print('Shape of phi_I after reshape', self.phi_I.get_shape())\n",
        "\n",
        "        self.phi_I = tf.divide(self.phi_I,784.0)  \n",
        "        print('Shape of phi_I after division', self.phi_I.get_shape())\n",
        "\n",
        "        self.y_ssqrt = tf.multiply(tf.sign(self.phi_I),tf.sqrt(tf.abs(self.phi_I)+1e-12))\n",
        "        print('Shape of y_ssqrt', self.y_ssqrt.get_shape())\n",
        "\n",
        "        self.z_l2 = tf.nn.l2_normalize(self.y_ssqrt, dim=1)\n",
        "        print('Shape of z_l2', self.z_l2.get_shape())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def fc_layers(self):\n",
        "\n",
        "        with tf.name_scope('fc-new') as scope:\n",
        "\n",
        "            fc3w = tf.get_variable('weights', [512*512, 100], initializer=tf.contrib.layers.xavier_initializer(), trainable=True)\n",
        "            fc3b = tf.Variable(tf.constant(1.0, shape=[100], dtype=tf.float32), name='biases', trainable=True)\n",
        "            self.fc3l = tf.nn.bias_add(tf.matmul(self.z_l2, fc3w), fc3b)\n",
        "            self.last_layer_parameters += [fc3w, fc3b]\n",
        "            self.parameters += [fc3w, fc3b]\n",
        "\n",
        "    def load_weights(self, sess):\n",
        "        weights = np.load(self.weight_file)\n",
        "        keys = sorted(weights.keys())\n",
        "        for i, k in enumerate(keys):\n",
        "            removed_layer_variables = ['fc6_W','fc6_b','fc7_W','fc7_b','fc8_W','fc8_b']\n",
        "            if not k in removed_layer_variables:\n",
        "                print(k)\n",
        "                print(\"\",i, k, np.shape(weights[k]))\n",
        "                sess.run(self.parameters[i].assign(weights[k]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BIUw0jxgUixt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "a3046893-65b9-4950-910b-2fc94dd70685"
      },
      "cell_type": "code",
      "source": [
        "print(\"--Data Preprocessing--\")\n",
        "train_data = h5py.File('./new_train_488.h5', 'r')\n",
        "val_data = h5py.File('./new_val_224.h5', 'r')\n",
        "\n",
        "\n",
        "print('Input data read complete')\n",
        "\n",
        "X_train, Y_train = train_data['X'], train_data['Y']\n",
        "X_val, Y_val = val_data['X'], val_data['Y']\n",
        "\n",
        "print(\"Data shapes -- (train, val, test)\", X_train.shape, X_val.shape)\n",
        "X_train, Y_train = shuffle(X_train, Y_train)\n",
        "\n",
        "X_val, Y_val = shuffle(X_val, Y_val)\n",
        "#print(Y_train[0])\n",
        "print(\"Device placement on. Creating Session\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--Data Preprocessing--\n",
            "Input data read complete\n",
            "Data shapes -- (train, val, test) (1000, 488, 488, 3) (666, 224, 224, 3)\n",
            "Device placement on. Creating Session\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tSrMbCKHWXzd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "33f029bd-d8f6-46f0-a032-55e1d9f6b3a2"
      },
      "cell_type": "code",
      "source": [
        "#sess = tf.Session(config=tf.ConfigProto(log_device_placement=False))\n",
        "sess = tf.Session()\n",
        "#sess = tf.InteractiveSession()\n",
        "with tf.device('/gpu:0'):\n",
        "    print(\"--vgg deploying--\")\n",
        "    imgs = tf.placeholder(tf.float32, [None, 448, 448, 3])\n",
        "    target = tf.placeholder(\"float\", [None, 100])\n",
        "    #print 'Creating graph'\n",
        "    vgg = vgg16(imgs, 'vgg16_weights.npz', sess)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--vgg deploying--\n",
            "Adding Data Augmentation\n",
            "Shape of conv5_3 (?, 28, 28, 512)\n",
            "Shape of phi_I after einsum (?, 512, 512)\n",
            "Shape of phi_I after reshape (?, 262144)\n",
            "Shape of phi_I after division (?, 262144)\n",
            "Shape of y_ssqrt (?, 262144)\n",
            "WARNING:tensorflow:From <ipython-input-2-a31632814128>:245: calling l2_normalize (from tensorflow.python.ops.nn_impl) with dim is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "dim is deprecated, use axis instead\n",
            "Shape of z_l2 (?, 262144)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fHCmsaubWgWz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 935
        },
        "outputId": "46a65179-943f-40c0-edbd-84e2ada8d9d8"
      },
      "cell_type": "code",
      "source": [
        "    with tf.device(\"/gpu:0\"):\n",
        "        print('VGG network created')\n",
        "        \n",
        "        \n",
        "        # Defining other ops using Tensorflow\n",
        "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=vgg.fc3l, labels=target))\n",
        "        learning_rate_wft = tf.placeholder(tf.float32, shape=[])\n",
        "        learning_rate_woft = tf.placeholder(tf.float32, shape=[])\n",
        "    \n",
        "        optimizer = tf.train.MomentumOptimizer(learning_rate=0.9, momentum=0.9).minimize(loss)\n",
        "    \n",
        "    \n",
        "        correct_prediction = tf.equal(tf.argmax(vgg.fc3l,1), tf.argmax(target,1))\n",
        "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "    \n",
        "        num_correct_preds = tf.reduce_sum(tf.cast(correct_prediction, tf.float32))\n",
        "    \n",
        "        sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "        vgg.load_weights(sess)\n",
        "    \n",
        "        batch_size = 32\n",
        "    \n",
        "    \n",
        "        print('Starting training')\n",
        "    \n",
        "        lr = 1.0\n",
        "        base_lr = 1.0\n",
        "        break_training_epoch = 15\n",
        "        for epoch in range(100):\n",
        "            avg_cost = 0.\n",
        "            total_batch = int(1000/batch_size)\n",
        "            X_train, Y_train = shuffle(X_train, Y_train)\n",
        "            \n",
        "    \n",
        "            \n",
        "            # Uncomment following section if you want to break training at a particular epoch\n",
        "    \n",
        "            '''\n",
        "            if epoch==break_training_epoch:\n",
        "                last_layer_weights = []\n",
        "                for v in vgg.parameters:\n",
        "                    print(v)\n",
        "                    if v in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES):\n",
        "                        print('Printing Trainable Variables :', sess.run(v).shape)\n",
        "                        last_layer_weights.append(sess.run(v))\n",
        "                np.savez('last_layers_epoch_15.npz',last_layer_weights)\n",
        "                print(\"Last layer weights saved\")\n",
        "                break\n",
        "            '''\n",
        "    \n",
        "            for i in range(total_batch):\n",
        "                batch_xs, batch_ys = X_train[i*batch_size:i*batch_size+batch_size], Y_train[i*batch_size:i*batch_size+batch_size]\n",
        "                batch_xs = random_flip_right_to_left(batch_xs)\n",
        "    \n",
        "                #if epoch <= finetune_step:\n",
        "                start = time.time()\n",
        "                sess.run(optimizer, feed_dict={imgs: batch_xs, target: batch_ys})\n",
        "                if i%20==0:\n",
        "                    print('Last layer training, time to run optimizer for batch size:', batch_size,'is --> ',time.time()-start,'seconds')\n",
        "    \n",
        "    \n",
        "                cost = sess.run(loss, feed_dict={imgs: batch_xs, target: batch_ys})\n",
        "                if i % 100 == 0:\n",
        "                    #print ('Learning rate: ', (str(lr)))\n",
        "                    #if epoch <= finetune_step:\n",
        "                    #print(\"Training last layer of BCNN_DD\")\n",
        "    \n",
        "                    print(\"Epoch:\", '%03d' % (epoch+1), \"Step:\", '%03d' % i,\"Loss:\", str(cost))\n",
        "                    print(\"Training Accuracy -->\", sess.run(accuracy,feed_dict={imgs: batch_xs, target: batch_ys}))\n",
        "    \n",
        "            val_batch_size = 10\n",
        "            total_val_count = len(X_val)\n",
        "            correct_val_count = 0\n",
        "            val_loss = 0.0\n",
        "            total_val_batch = int(total_val_count/val_batch_size)\n",
        "            for i in range(total_val_batch):\n",
        "                batch_val_x, batch_val_y = X_val[i*val_batch_size:i*val_batch_size+val_batch_size], Y_val[i*val_batch_size:i*val_batch_size+val_batch_size]\n",
        "                val_loss += sess.run(loss, feed_dict={imgs: batch_val_x, target: batch_val_y})\n",
        "    \n",
        "                pred = sess.run(num_correct_preds, feed_dict = {imgs: batch_val_x, target: batch_val_y})\n",
        "                correct_val_count+=pred\n",
        "    \n",
        "            print(\"##############################\")\n",
        "            print(\"Validation Loss -->\", val_loss)\n",
        "            print(\"correct_val_count, total_val_count\", correct_val_count, total_val_count)\n",
        "            print(\"Validation Data Accuracy -->\", 100.0*correct_val_count/(1.0*total_val_count))\n",
        "            print(\"##############################\")\n",
        "    \n",
        "            \n",
        "    \n",
        "        test_data = h5py.File('./new_test.h5', 'r')\n",
        "        X_test, Y_test = test_data['X'], test_data['Y']\n",
        "        total_test_count = len(X_test)\n",
        "        correct_test_count = 0\n",
        "        test_batch_size = 10\n",
        "        total_test_batch = int(total_test_count/test_batch_size)\n",
        "        for i in range(total_test_batch):\n",
        "            batch_test_x, batch_test_y = X_test[i*test_batch_size:i*test_batch_size+test_batch_size], Y_test[i*test_batch_size:i*test_batch_size+test_batch_size]\n",
        "            \n",
        "            pred = sess.run(num_correct_preds, feed_dict = {imgs: batch_test_x, target: batch_test_y})\n",
        "            correct_test_count+=pred\n",
        "    \n",
        "        print(\"##############################\")\n",
        "        print(\"correct_test_count, total_test_count\", correct_test_count, total_test_count)\n",
        "        print(\"Test Data Accuracy -->\", 100.0*correct_test_count/(1.0*total_test_count))\n",
        "        print(\"##############################\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "VGG network created\n",
            "conv1_1_W\n",
            " 0 conv1_1_W (3, 3, 3, 64)\n",
            "conv1_1_b\n",
            " 1 conv1_1_b (64,)\n",
            "conv1_2_W\n",
            " 2 conv1_2_W (3, 3, 64, 64)\n",
            "conv1_2_b\n",
            " 3 conv1_2_b (64,)\n",
            "conv2_1_W\n",
            " 4 conv2_1_W (3, 3, 64, 128)\n",
            "conv2_1_b\n",
            " 5 conv2_1_b (128,)\n",
            "conv2_2_W\n",
            " 6 conv2_2_W (3, 3, 128, 128)\n",
            "conv2_2_b\n",
            " 7 conv2_2_b (128,)\n",
            "conv3_1_W\n",
            " 8 conv3_1_W (3, 3, 128, 256)\n",
            "conv3_1_b\n",
            " 9 conv3_1_b (256,)\n",
            "conv3_2_W\n",
            " 10 conv3_2_W (3, 3, 256, 256)\n",
            "conv3_2_b\n",
            " 11 conv3_2_b (256,)\n",
            "conv3_3_W\n",
            " 12 conv3_3_W (3, 3, 256, 256)\n",
            "conv3_3_b\n",
            " 13 conv3_3_b (256,)\n",
            "conv4_1_W\n",
            " 14 conv4_1_W (3, 3, 256, 512)\n",
            "conv4_1_b\n",
            " 15 conv4_1_b (512,)\n",
            "conv4_2_W\n",
            " 16 conv4_2_W (3, 3, 512, 512)\n",
            "conv4_2_b\n",
            " 17 conv4_2_b (512,)\n",
            "conv4_3_W\n",
            " 18 conv4_3_W (3, 3, 512, 512)\n",
            "conv4_3_b\n",
            " 19 conv4_3_b (512,)\n",
            "conv5_1_W\n",
            " 20 conv5_1_W (3, 3, 512, 512)\n",
            "conv5_1_b\n",
            " 21 conv5_1_b (512,)\n",
            "conv5_2_W\n",
            " 22 conv5_2_W (3, 3, 512, 512)\n",
            "conv5_2_b\n",
            " 23 conv5_2_b (512,)\n",
            "conv5_3_W\n",
            " 24 conv5_3_W (3, 3, 512, 512)\n",
            "conv5_3_b\n",
            " 25 conv5_3_b (512,)\n",
            "Starting training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zVan_l79Wyrf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "a074792a-4a10-4d42-f14f-1bb254e20520"
      },
      "cell_type": "code",
      "source": [
        "!wget https://www.cs.toronto.edu/~frossard/vgg16/vgg16_weights.npz"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-04-11 01:39:35--  https://www.cs.toronto.edu/~frossard/vgg16/vgg16_weights.npz\n",
            "Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n",
            "Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 553436134 (528M)\n",
            "Saving to: ‘vgg16_weights.npz’\n",
            "\n",
            "vgg16_weights.npz    32%[=====>              ] 172.93M  4.48MB/s    eta 1m 42s "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "vgg16_weights.npz   100%[===================>] 527.80M  1.98MB/s    in 2m 22s  \n",
            "\n",
            "2018-04-11 01:41:58 (3.71 MB/s) - ‘vgg16_weights.npz’ saved [553436134/553436134]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}